{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42821cc1fb6049e49561b694cc0f794e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2de6821c5c814450b0a1afcc3530de6a",
              "IPY_MODEL_90917ec79b704b9faab5218f0816b000",
              "IPY_MODEL_69d1a569736342f59aa746c86fd80aaf"
            ],
            "layout": "IPY_MODEL_0f7fcb8c4ba9444c81dd8062f4d375dc"
          }
        },
        "2de6821c5c814450b0a1afcc3530de6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2658f6b26e3246ed94a7ef17d12c42cb",
            "placeholder": "​",
            "style": "IPY_MODEL_15c09cdcf0e3427fbf3307916ceca507",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "90917ec79b704b9faab5218f0816b000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7f79beca244efebfef9b4f9d1d7843",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25dba4c070304bb9ad45cf8289a238c6",
            "value": 2
          }
        },
        "69d1a569736342f59aa746c86fd80aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe8b525a1146409e9983a2014766f279",
            "placeholder": "​",
            "style": "IPY_MODEL_0ff3767872384d2ba9a682119540420e",
            "value": " 2/2 [00:38&lt;00:00, 18.40s/it]"
          }
        },
        "0f7fcb8c4ba9444c81dd8062f4d375dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2658f6b26e3246ed94a7ef17d12c42cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c09cdcf0e3427fbf3307916ceca507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe7f79beca244efebfef9b4f9d1d7843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dba4c070304bb9ad45cf8289a238c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe8b525a1146409e9983a2014766f279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff3767872384d2ba9a682119540420e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ec42c1e7eaf4fb0adf9ff3ddb89bd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_502fcdfaa53d499babf60bcab31f98e6",
              "IPY_MODEL_796ce2a9ca9f4e71856c29871f4c2345",
              "IPY_MODEL_1c7848216a5f496aabfeb58d7f8b17af"
            ],
            "layout": "IPY_MODEL_e655754ca7ec4197862377e379e0b984"
          }
        },
        "502fcdfaa53d499babf60bcab31f98e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd194b867a954c0abb9162831f69d148",
            "placeholder": "​",
            "style": "IPY_MODEL_d12d3b2b69a44322b4152cb5ab819c99",
            "value": "Generating train split: "
          }
        },
        "796ce2a9ca9f4e71856c29871f4c2345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ef96f9a30c461289c8805da775c27f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b13d1fb3eac34d6f84a9d70fd66b714f",
            "value": 1
          }
        },
        "1c7848216a5f496aabfeb58d7f8b17af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f59d0d91c07478fa0eb88306471d59d",
            "placeholder": "​",
            "style": "IPY_MODEL_ec174e0791264fbe9433f0ca518ea0e4",
            "value": " 79/0 [00:00&lt;00:00, 1496.59 examples/s]"
          }
        },
        "e655754ca7ec4197862377e379e0b984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd194b867a954c0abb9162831f69d148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d12d3b2b69a44322b4152cb5ab819c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ef96f9a30c461289c8805da775c27f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b13d1fb3eac34d6f84a9d70fd66b714f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f59d0d91c07478fa0eb88306471d59d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec174e0791264fbe9433f0ca518ea0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a0f3a1dff1f4868927fa29c3c2dd8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_913acec0d02d4e5c8cea25b1e118495b",
              "IPY_MODEL_3e92448a09dd49c5bfb1076daa17cb99",
              "IPY_MODEL_647869f286fe4508a19eda2e6a7baebe"
            ],
            "layout": "IPY_MODEL_0f0095ccd5f3447aa0254c4220743481"
          }
        },
        "913acec0d02d4e5c8cea25b1e118495b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bd43f5607fb48b7b1fac3ef833e435b",
            "placeholder": "​",
            "style": "IPY_MODEL_7b4c2160756b45dc9589bceb97b009db",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3e92448a09dd49c5bfb1076daa17cb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c67109afcf4eb1a3e4563d0d01479c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2b3f505defc491e8abb1502b3da4243",
            "value": 2
          }
        },
        "647869f286fe4508a19eda2e6a7baebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ed75d870194237b32f31c8651ba96c",
            "placeholder": "​",
            "style": "IPY_MODEL_b1aa5229dcc146ef95d91a96af1a610c",
            "value": " 2/2 [00:35&lt;00:00, 16.99s/it]"
          }
        },
        "0f0095ccd5f3447aa0254c4220743481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd43f5607fb48b7b1fac3ef833e435b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4c2160756b45dc9589bceb97b009db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35c67109afcf4eb1a3e4563d0d01479c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b3f505defc491e8abb1502b3da4243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50ed75d870194237b32f31c8651ba96c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1aa5229dcc146ef95d91a96af1a610c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cafebbc0afd4423a2b7128804ed624c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4abd9caf7c994b1bb87ad68bdf09f4c2",
              "IPY_MODEL_4b84c97ad8f841a5be5dddfa2acd92c3",
              "IPY_MODEL_3af39c0edd834eed90b2cf0138ce3894"
            ],
            "layout": "IPY_MODEL_d3576b44d89a449b9ed8efb1cd94699e"
          }
        },
        "4abd9caf7c994b1bb87ad68bdf09f4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ffce64625974681b642def452b491f9",
            "placeholder": "​",
            "style": "IPY_MODEL_ac5ca25f818547a2a056c9515983364e",
            "value": "Map: 100%"
          }
        },
        "4b84c97ad8f841a5be5dddfa2acd92c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d5fa26830b469d8d736131a464d73c",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5048cdc1dca4215a4d93de6504289f0",
            "value": 71
          }
        },
        "3af39c0edd834eed90b2cf0138ce3894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_432a69b37fa6453099a0f31500fcb3df",
            "placeholder": "​",
            "style": "IPY_MODEL_0b41727b85b2459b97f88ce1114c3c47",
            "value": " 71/71 [00:12&lt;00:00,  6.03 examples/s]"
          }
        },
        "d3576b44d89a449b9ed8efb1cd94699e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ffce64625974681b642def452b491f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5ca25f818547a2a056c9515983364e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7d5fa26830b469d8d736131a464d73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5048cdc1dca4215a4d93de6504289f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "432a69b37fa6453099a0f31500fcb3df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b41727b85b2459b97f88ce1114c3c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c8f341c4f94d888ca75907d48332ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0af35ce5c86e4b6fbe0460ca86145045",
              "IPY_MODEL_a0763a02632e4fb4b159dc65a3b0c8e1",
              "IPY_MODEL_788aac0174184dc780af9806c06dcc4a"
            ],
            "layout": "IPY_MODEL_67ba0ac430a94a7aab502ef88e121733"
          }
        },
        "0af35ce5c86e4b6fbe0460ca86145045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d095bc1c758d48d1b8805f774950c8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c4f5f7210b4c8ebcdeed955ea77b33",
            "value": "Map: 100%"
          }
        },
        "a0763a02632e4fb4b159dc65a3b0c8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4caba4df19c4ef7986ae57edd7d1da1",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de880ef7875a4e1cb8b0c30be5b10dba",
            "value": 8
          }
        },
        "788aac0174184dc780af9806c06dcc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd2d2dc3fe2c4219902942e44c63a47f",
            "placeholder": "​",
            "style": "IPY_MODEL_2263a363709940798501c7ea7ff829f7",
            "value": " 8/8 [00:01&lt;00:00,  5.33 examples/s]"
          }
        },
        "67ba0ac430a94a7aab502ef88e121733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d095bc1c758d48d1b8805f774950c8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c4f5f7210b4c8ebcdeed955ea77b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4caba4df19c4ef7986ae57edd7d1da1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de880ef7875a4e1cb8b0c30be5b10dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd2d2dc3fe2c4219902942e44c63a47f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2263a363709940798501c7ea7ff829f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe032768b380479f94bfb1c3dcf93daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ae6ee35ddda45568dca77dec0d652e9",
              "IPY_MODEL_c2e36f4fb0e1433da49367ea89a8e962",
              "IPY_MODEL_5890d46a58d3432299958e139c8001eb"
            ],
            "layout": "IPY_MODEL_cb32363bfc5f4e5eaf95ffa0003eab99"
          }
        },
        "1ae6ee35ddda45568dca77dec0d652e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e3b3fbe12f43789d1beb1addab5276",
            "placeholder": "​",
            "style": "IPY_MODEL_854fd491d1f447e2b4411986045a1afa",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c2e36f4fb0e1433da49367ea89a8e962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab88575135084b15a71403ed9b78d7f9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac50bd830c594a27af8539a6a78168b0",
            "value": 2
          }
        },
        "5890d46a58d3432299958e139c8001eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ab06350a1744a3c9314d659a9753ca5",
            "placeholder": "​",
            "style": "IPY_MODEL_afa1d19d5f9e48d8874d9508793f369a",
            "value": " 2/2 [00:30&lt;00:00, 15.10s/it]"
          }
        },
        "cb32363bfc5f4e5eaf95ffa0003eab99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e3b3fbe12f43789d1beb1addab5276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "854fd491d1f447e2b4411986045a1afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab88575135084b15a71403ed9b78d7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac50bd830c594a27af8539a6a78168b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ab06350a1744a3c9314d659a9753ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa1d19d5f9e48d8874d9508793f369a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathsaraniii/neural-networks-and-deep-learning/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz_Zzjy6WylB",
        "outputId": "1edfffe0-01cf-4265-ede8-78a76bb807ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft accelerate bitsandbytes datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoltgcxOkpCC",
        "outputId": "736b0bdb-191d-408c-a454-f1fcc03bb710"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xePpJr-y2Szz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "# Set up BitsAndBytesConfig for quantization (e.g., 4-bit)\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# Load the model with quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "42821cc1fb6049e49561b694cc0f794e",
            "2de6821c5c814450b0a1afcc3530de6a",
            "90917ec79b704b9faab5218f0816b000",
            "69d1a569736342f59aa746c86fd80aaf",
            "0f7fcb8c4ba9444c81dd8062f4d375dc",
            "2658f6b26e3246ed94a7ef17d12c42cb",
            "15c09cdcf0e3427fbf3307916ceca507",
            "fe7f79beca244efebfef9b4f9d1d7843",
            "25dba4c070304bb9ad45cf8289a238c6",
            "fe8b525a1146409e9983a2014766f279",
            "0ff3767872384d2ba9a682119540420e"
          ]
        },
        "id": "nmZ2y0rBlNPt",
        "outputId": "d5af06e9-793e-479a-e2df-9399651e85d1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42821cc1fb6049e49561b694cc0f794e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w7Yh7mG2Xi4",
        "outputId": "ee133409-b9d1-44a7-bb1b-85e8d3c9ea06"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3fjqZc02p4v",
        "outputId": "8d78f521-bfe6-4a2e-a572-1530cf714b55"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example of text extraction from a research paper (you can replace this with text from your paper)\n",
        "text = \"\"\"\n",
        "# DualPipe\n",
        "DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\n",
        "\n",
        "Pipeline Bubbles and Memory Usage Comparison\n",
        "\n",
        "| Method    | Bubble                  | Parameter | Activation |\n",
        "|:---------:|:-----------------------:|:---------:|:----------:|\n",
        "| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\n",
        "| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\n",
        "| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\n",
        "\n",
        "𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\n",
        "\n",
        "### About\n",
        "A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\n",
        "\n",
        "`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\n",
        "\n",
        "# Profiling Data in DeepSeek Infra\n",
        "Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.\n",
        "\n",
        "## Training\n",
        "The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\n",
        "\n",
        "## Inference\n",
        "### Prefilling\n",
        "For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\n",
        "\n",
        "### Decoding\n",
        "For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\n",
        "\n",
        "# Expert Parallelism Load Balancer (EPLB)\n",
        "\n",
        "When using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\n",
        "\n",
        "To facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\n",
        "\n",
        "## The Algorithm\n",
        "\n",
        "The load balancing algorithm comes with two policies used for different cases.\n",
        "\n",
        "## Hierarchical Load Balancing\n",
        "\n",
        "When the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\n",
        "\n",
        "### Global Load Balancing\n",
        "\n",
        "In other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\n",
        "\n",
        "# Fire-Flyer File system\n",
        "The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\n",
        "\n",
        "- Performance and Usability\n",
        "\n",
        "    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\n",
        "    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\n",
        "    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\n",
        "\n",
        "- Diverse Workloads\n",
        "\n",
        "    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\n",
        "    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\n",
        "    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\n",
        "    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\n",
        "\n",
        "## Performance\n",
        "1. Peak throughput\n",
        "\n",
        "The following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\n",
        "\n",
        "2. GraySort\n",
        "\n",
        "We evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\n",
        "\n",
        "The test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\n",
        "\n",
        "3. KVCache\n",
        "\n",
        "KVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extracting sentences\n",
        "for sentence in doc.sents:\n",
        "    print(sentence.text)\n",
        "\n",
        "# Extracting named entities (e.g., important terms, authors, technologies, etc.)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySuzRatN3BL6",
        "outputId": "788f5908-ffde-4cfd-a9de-04ff57058cde"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# DualPipe\n",
            "DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report.\n",
            "It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles.\n",
            "For detailed information on computation-communication overlap, please refer to the profile data.\n",
            "\n",
            "\n",
            "Pipeline Bubbles and Memory Usage Comparison\n",
            "\n",
            "| Method    | Bubble                  | Parameter | Activation |\n",
            "|:---------:|:-----------------------:|:---------:|:----------:|\n",
            "| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\n",
            "| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\n",
            "| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\n",
            "\n",
            "𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\n",
            "\n",
            "\n",
            "### About\n",
            "A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\n",
            "\n",
            "`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.\n",
            "`\n",
            "\n",
            "# Profiling Data in DeepSeek Infra\n",
            "Here, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details.\n",
            "The profiling data was captured using the PyTorch Profiler.\n",
            "After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser).\n",
            "Notice that we simulate an absolutely balanced MoE routing strategy for profiling.\n",
            "\n",
            "\n",
            "## Training\n",
            "The training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe.\n",
            "Each chunk contains 4 MoE (Mixture of Experts) layers.\n",
            "The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length.\n",
            "And the PP communication is not included during profilng for simplicity.\n",
            "\n",
            "\n",
            "## Inference\n",
            "### Prefilling\n",
            "For prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU.\n",
            "In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\n",
            "\n",
            "\n",
            "### Decoding\n",
            "For decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU.\n",
            "Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication.\n",
            "However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished.\n",
            "For more information about the all-to-all implementation, please refer to DeepEP.\n",
            "\n",
            "\n",
            "# Expert Parallelism Load Balancer (EPLB)\n",
            "\n",
            "\n",
            "When using expert parallelism (EP), different experts are assigned to different GPUs.\n",
            "Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced.\n",
            "As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts.\n",
            "Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs.\n",
            "Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\n",
            "\n",
            "\n",
            "To facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py.\n",
            "The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads.\n",
            "Note that the exact method to predict the loads of experts is out of this repo's scope.\n",
            "A common method is to use moving average of historical statistics.\n",
            "\n",
            "\n",
            "## The Algorithm\n",
            "\n",
            "The load balancing algorithm comes with two policies used for different cases.\n",
            "\n",
            "\n",
            "## Hierarchical Load Balancing\n",
            "\n",
            "When the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing.\n",
            "We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced.\n",
            "Then, we replicate the experts within each node.\n",
            "Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
            "The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\n",
            "\n",
            "### Global Load Balancing\n",
            "\n",
            "In other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs.\n",
            "This policy can be adopted in decoding stage with a larger expert-parallel size.\n",
            "\n",
            "\n",
            "# Fire-Flyer File system\n",
            "The Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads.\n",
            "It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications.\n",
            "Key features and benefits of 3FS include:\n",
            "\n",
            "- Performance and Usability\n",
            "\n",
            "    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\n",
            "    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\n",
            "    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB).\n",
            "The file interface is well known and used everywhere.\n",
            "There is no need to learn a new storage API.\n",
            "\n",
            "- Diverse Workloads\n",
            "\n",
            "    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\n",
            "    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\n",
            "    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\n",
            "    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\n",
            "\n",
            "\n",
            "## Performance\n",
            "1.\n",
            "Peak throughput\n",
            "\n",
            "The following figure demonstrates the throughput of read stress test on a large 3FS cluster.\n",
            "This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs.\n",
            "Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC.\n",
            "The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\n",
            "\n",
            "\n",
            "2.\n",
            "GraySort\n",
            "\n",
            "We evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets.\n",
            "Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting.\n",
            "Both phases read/write data from/to 3FS.\n",
            "\n",
            "\n",
            "The test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node).\n",
            "Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\n",
            "\n",
            "3. KVCache\n",
            "\n",
            "KVCache is a technique used to optimize the LLM inference process.\n",
            "It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers.\n",
            "The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/\n",
            "s.\n",
            "The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.\n",
            "\n",
            "DualPipe PRODUCT\n",
            "DualPipe PRODUCT\n",
            "Pipeline Bubbles PERSON\n",
            "Memory Usage Comparison ORG\n",
            "Method NORP\n",
            "Bubble                   LOC\n",
            "Parameter PERSON\n",
            "Activation PERSON\n",
            "|:---------:|:-----------------------:|:---------:|:----------:|\n",
            "| PERSON\n",
            "1F1B CARDINAL\n",
            "PP          LOC\n",
            "ZB1P       PRODUCT\n",
            "PP          LOC\n",
            "DualPipe PRODUCT\n",
            "𝐹&𝐵 ORG\n",
            "two CARDINAL\n",
            "### MONEY\n",
            "V3 CARDINAL\n",
            "DualPipe PRODUCT\n",
            "Jiashi Li PERSON\n",
            "Chengqi Deng PERSON\n",
            "Wenfeng Liang PERSON\n",
            "# Profiling Data ORG\n",
            "the PyTorch Profiler ORG\n",
            "Chrome PERSON\n",
            "Edge ORG\n",
            "## MONEY\n",
            "DualPipe PRODUCT\n",
            "4 CARDINAL\n",
            "MoE DATE\n",
            "DeepSeek-V3 PRODUCT\n",
            "4 CARDINAL\n",
            "PP NORP\n",
            "## Inference MONEY\n",
            "### MONEY\n",
            "V3 CARDINAL\n",
            "4 CARDINAL\n",
            "16 CARDINAL\n",
            "GPU ORG\n",
            "two CARDINAL\n",
            "two CARDINAL\n",
            "### Decoding MONEY\n",
            "EP128 PERSON\n",
            "4 CARDINAL\n",
            "128 CARDINAL\n",
            "GPU ORG\n",
            "two CARDINAL\n",
            "GPU ORG\n",
            "RDMA ORG\n",
            "GPU ORG\n",
            "# CARDINAL\n",
            "EPLB ORG\n",
            "DeepSeek-V3 PRODUCT\n",
            "DeepSeek-V3 PRODUCT\n",
            "eplb.py DATE\n",
            "## MONEY\n",
            "two CARDINAL\n",
            "## Hierarchical MONEY\n",
            "first ORDINAL\n",
            "### MONEY\n",
            "Fire-Flyer File ORG\n",
            "The Fire-Flyer File System FAC\n",
            "AI ORG\n",
            "RDMA ORG\n",
            "3FS FAC\n",
            "thousands CARDINAL\n",
            "hundreds CARDINAL\n",
            "API ORG\n",
            "Inference Provides ORG\n",
            "DRAM ORG\n",
            "## MONEY\n",
            "1 CARDINAL\n",
            "3FS PRODUCT\n",
            "180 CARDINAL\n",
            "2×200Gbps CARDINAL\n",
            "InfiniBand PRODUCT\n",
            "sixteen 14TiB DATE\n",
            "node PERSON\n",
            "1x200Gbps CARDINAL\n",
            "NIC ORG\n",
            "TiB GPE\n",
            "2 CARDINAL\n",
            "GraySort PRODUCT\n",
            "two CARDINAL\n",
            "1 CARDINAL\n",
            "2 CARDINAL\n",
            "3FS FAC\n",
            "25 CARDINAL\n",
            "2 CARDINAL\n",
            "NUMA ORG\n",
            "1 CARDINAL\n",
            "2×400Gbps CARDINAL\n",
            "50 CARDINAL\n",
            "2 CARDINAL\n",
            "192 CARDINAL\n",
            "2.2 CARDINAL\n",
            "TiB RAM ORG\n",
            "1×200 CARDINAL\n",
            "Gbps NIC/ ORG\n",
            "110.5 TiB CARDINAL\n",
            "8,192 CARDINAL\n",
            "30 minutes and 14 seconds TIME\n",
            "3.66 CARDINAL\n",
            "TiB GPE\n",
            "3 CARDINAL\n",
            "KVCache ORG\n",
            "KVCache ORG\n",
            "LLM ORG\n",
            "KVCache ORG\n",
            "up to 40 CARDINAL\n",
            "GiB WORK_OF_ART\n",
            "GC ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained QA model from Hugging Face\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Define the context (extracted text from the research paper)\n",
        "context = \"\"\"\n",
        "\n",
        "DualPipe\n",
        "DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report.\n",
        "It achieves full overlap of forward and backward computation-communication phases, reducing pipeline bubbles.\n",
        "\n",
        "For detailed information on computation-communication overlap, refer to the profile data.\n",
        "\n",
        "Pipeline Bubbles and Memory Usage Comparison\n",
        "Method\tBubble Formula\tParameter\tActivation\n",
        "1F1B\t(PP-1)(𝐹+𝐵)\t1×\tPP\n",
        "ZB1P\t(PP-1)(𝐹+𝐵-2𝑊)\t1×\tPP\n",
        "DualPipe\t(PP/2-1)(𝐹&𝐵+𝐵-3𝑊)\t2×\tPP+1\n",
        "Legend:\n",
        "\n",
        "𝐹 → Execution time of a forward chunk\n",
        "𝐵 → Execution time of a full backward chunk\n",
        "𝑊 → Execution time of a \"backward for weights\" chunk\n",
        "𝐹&𝐵 → Execution time of two mutually overlapped forward and backward chunks\n",
        "About DualPipe\n",
        "A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.\n",
        "\n",
        "Developed by:\n",
        "\n",
        "Jiashi Li\n",
        "Chengqi Deng\n",
        "Wenfeng Liang\n",
        "Profiling Data in DeepSeek Infra\n",
        "The PyTorch Profiler was used to capture training and inference profiling data.\n",
        "To visualize it:\n",
        "\n",
        "Download the profiling data.\n",
        "Open chrome://tracing (Chrome) or edge://tracing (Edge).\n",
        "Training\n",
        "Demonstrates overlapping strategy for forward and backward chunks in DualPipe.\n",
        "Uses 4 MoE layers per chunk.\n",
        "Parallel configuration: EP64, TP1, sequence length 4K.\n",
        "PP communication not included for simplicity.\n",
        "Inference\n",
        "Prefilling\n",
        "Configuration: EP32, TP1\n",
        "Prompt length: 4K, batch size: 16K tokens per GPU\n",
        "Uses two micro-batches to overlap computation and communication.\n",
        "Decoding\n",
        "Configuration: EP128, TP1\n",
        "Prompt length: 4K, batch size: 128 requests per GPU\n",
        "Uses two micro-batches, but GPU SMs remain free after issuing RDMA messages.\n",
        "Expert Parallelism Load Balancer (EPLB)\n",
        "When using expert parallelism (EP):\n",
        "\n",
        "Different experts are assigned to different GPUs.\n",
        "Load balancing is ensured by duplicating heavy-loaded experts.\n",
        "Uses a group-limited expert routing strategy to minimize inter-node data traffic.\n",
        "Load Balancing Policies:\n",
        "\n",
        "Hierarchical Load Balancing → Used when expert groups can be evenly packed into nodes.\n",
        "Global Load Balancing → Used when experts are replicated globally without considering groups.\n",
        "Fire-Flyer File System (3FS)\n",
        "3FS is a high-performance distributed file system designed for AI training and inference.\n",
        "It leverages SSDs + RDMA networks for efficient storage.\n",
        "\n",
        "Key Features:\n",
        "✅ High Throughput – Uses thousands of SSDs across hundreds of nodes.\n",
        "✅ Strong Consistency – Implements CRAQ (Chain Replication with Apportioned Queries).\n",
        "✅ Supports Checkpointing, KVCache, Random Access for training & inference.\n",
        "\n",
        "Performance Benchmarks\n",
        "1. Read Stress Test\n",
        "180 storage nodes (each with 16 NVMe SSDs).\n",
        "500+ clients, each with 200Gbps NICs.\n",
        "Achieved 6.6 TiB/s read throughput under background training load.\n",
        "2. GraySort Benchmark\n",
        "Tested on 25 storage nodes + 50 compute nodes.\n",
        "Sorted 110.5 TiB in 30 min 14 sec.\n",
        "Throughput: 3.66 TiB/min.\n",
        "3. KVCache for LLM Inference\n",
        "Caches key-value vectors from previous tokens to reduce redundant computations.\n",
        "Peak read throughput: 40 GiB/s.\n",
        "\"\"\"\n",
        "\n",
        "# Example of a question you generated\n",
        "questions = [\"how does DualPipe improve computation in machine learning DualPipe?\",\"What is DualPipe?\",\" What is the purpose of the table 'Pipeline Bubbles and Memory Usage Comparison'?\",\"Explain the terms '𝐹', '𝐵', and '𝑊' used in the DualPipe algorithm.\",\"Who developed DualPipe?\",\"What is the role of the PyTorch Profiler in DeepSeek-V3?\",\"How does the training profile data in DeepSeek-V3 optimize performance?\",\"What is the purpose of expert parallelism (EP) in DeepSeek-V3?\",\"What is the difference between Hierarchical and Global Load Balancing in the load balancing algorithm?\",\"How does the Fire-Flyer File System (3FS) support large-scale AI workloads?\",\"What is KVCache and how does it optimize the LLM inference process?\",\"What performance results were achieved with the Fire-Flyer File System (3FS)?\",\"What was the setup for the GraySort benchmark test using the Fire-Flyer File System?\"]\n",
        "\n",
        "for question in questions:\n",
        "  # Get the answer using the QA model\n",
        "  result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "  # Print the answer\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Answer: {result['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHqJcgB_5xSE",
        "outputId": "0559b3c4-566d-4a82-fb22-91a270f459dc"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: how does DualPipe improve computation in machine learning DualPipe?\n",
            "Answer: reducing pipeline bubbles\n",
            "Question: What is DualPipe?\n",
            "Answer: an innovative bidirectional pipeline parallelism algorithm\n",
            "Question:  What is the purpose of the table 'Pipeline Bubbles and Memory Usage Comparison'?\n",
            "Answer: reducing pipeline bubbles\n",
            "Question: Explain the terms '𝐹', '𝐵', and '𝑊' used in the DualPipe algorithm.\n",
            "Answer: \tPP\n",
            "ZB1P\t\n",
            "Question: Who developed DualPipe?\n",
            "Answer: \n",
            "\n",
            "Jiashi Li\n",
            "Question: What is the role of the PyTorch Profiler in DeepSeek-V3?\n",
            "Answer: capture training and inference profiling data\n",
            "Question: How does the training profile data in DeepSeek-V3 optimize performance?\n",
            "Answer: background training load\n",
            "Question: What is the purpose of expert parallelism (EP) in DeepSeek-V3?\n",
            "Answer: Different experts are assigned to different GPUs\n",
            "Question: What is the difference between Hierarchical and Global Load Balancing in the load balancing algorithm?\n",
            "Answer: Used when expert groups can be evenly packed into nodes\n",
            "Question: How does the Fire-Flyer File System (3FS) support large-scale AI workloads?\n",
            "Answer: leverages SSDs + RDMA networks\n",
            "Question: What is KVCache and how does it optimize the LLM inference process?\n",
            "Answer: key-value vectors from previous tokens to reduce redundant computations\n",
            "Question: What performance results were achieved with the Fire-Flyer File System (3FS)?\n",
            "Answer: 6.6 TiB/s read throughput\n",
            "Question: What was the setup for the GraySort benchmark test using the Fire-Flyer File System?\n",
            "Answer: 25 storage nodes + 50 compute nodes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the modified QA pairs\n",
        "qa_data = [\n",
        "    {\"instruction\": \"How does the DeepSeek-V3 model achieve both high performance and economical training as an open-source model?\", \"input\": \"\", \"output\": \"DeepSeekMoE\"},\n",
        "    {\"instruction\": \"What are the key features introduced in DeepSeek-V3 that differentiate it from previous versions?\", \"input\": \"\", \"output\": \"attention and Feed-Forward Network\"},\n",
        "    {\"instruction\": \"How does the DeepSeek-V3 model compare to standard Transformer models and other well-known LLMs like LLaMA?\", \"input\": \"\", \"output\": \"its core principle still resembles\"},\n",
        "    {\"instruction\": \"Why is prior knowledge of LLaMA and other large language models useful for understanding DeepSeek-V3?\", \"input\": \"\", \"output\": \"It will be incredibly helpful\"},\n",
        "    {\"instruction\": \"What are the main architectural components of DeepSeek-V3?\", \"input\": \"\", \"output\": \"DeepSeekMoE\"},\n",
        "    {\"instruction\": \"How does DeepSeek-V3 improve upon the previous DeepSeek-V2 model?\", \"input\": \"\", \"output\": \"Group Relative Policy Optimization\"},\n",
        "    {\"instruction\": \"In what ways does DeepSeek-V3 enhance the standard Transformer architecture?\", \"input\": \"\", \"output\": \"attention and Feed-Forward Network\"},\n",
        "    {\"instruction\": \"What is Multi-Head Latent Attention (MLA), and how does it improve speed and memory efficiency?\", \"input\": \"\", \"output\": \"compressing the input vector\"},\n",
        "    {\"instruction\": \"How does MLA utilize compression techniques similar to PCA?\", \"input\": \"\", \"output\": \"By storing a compressed vector for the KV cache\"},\n",
        "    {\"instruction\": \"Why does DeepSeek-V3 require an additional weight matrix for compression?\", \"input\": \"\", \"output\": \"human cannot compress it manually\"},\n",
        "    {\"instruction\": \"What issue arises with RoPE when applied to compressed vectors, and how is it addressed?\", \"input\": \"\", \"output\": \"not mathematically compatible\"},\n",
        "    {\"instruction\": \"How does DeepSeekMoE improve model performance compared to a single large FFN?\", \"input\": \"\", \"output\": \"Multi-Head Latent Attention\"},\n",
        "    {\"instruction\": \"What is the role of expert selection in DeepSeekMoE?\", \"input\": \"\", \"output\": \"to be activated and they contribute to make output\"},\n",
        "    {\"instruction\": \"How does the model determine which experts to activate?\", \"input\": \"\", \"output\": \"Dependent on the input sequence\"},\n",
        "    {\"instruction\": \"What mathematical approach is used to compute the similarity between input tokens and expert domains?\", \"input\": \"\", \"output\": \"DualPipe\"},\n",
        "    {\"instruction\": \"What is Multi-Token Prediction (MTP), and why is it important?\", \"input\": \"\", \"output\": \"restricts the efficiency and the speed of convergence during training\"},\n",
        "    {\"instruction\": \"How does DeepSeek-V3’s sequential MTP differ from previous parallel MTP approaches?\", \"input\": \"\", \"output\": \"Multi-Head Latent Attention\"},\n",
        "    {\"instruction\": \"Why does the structure of MTP modules resemble an RNN?\", \"input\": \"\", \"output\": \"preserve hidden states of nodes\"},\n",
        "    {\"instruction\": \"What benefits does MTP provide in terms of training efficiency and learning speed?\", \"input\": \"\", \"output\": \"convergence\"},\n",
        "    {\"instruction\": \"Why does DeepSeek-V3 generate only one additional token during training?\", \"input\": \"\", \"output\": \"computational cost\"},\n",
        "    {\"instruction\": \"What challenges did DeepSeek researchers face due to using NVIDIA H800 GPUs instead of H100s?\", \"input\": \"\", \"output\": \"Multi-Head Latent Attention\"},\n",
        "    {\"instruction\": \"What is DualPipe, and how does it help accelerate model training?\", \"input\": \"\", \"output\": \"makes the device 7 start training with other batch data in the opposite direction\"},\n",
        "    {\"instruction\": \"How does DualPipe reduce GPU communication overhead?\", \"input\": \"\", \"output\": \"combine them as a chunk and continuously copy them together on other devices\"},\n",
        "    {\"instruction\": \"Why is minimizing 'bubble' time between GPU operations important?\", \"input\": \"\", \"output\": \"training inefficiencies\"},\n",
        "    {\"instruction\": \"How does DualPipe utilize bidirectional data flow to improve efficiency?\", \"input\": \"\", \"output\": \"combine them as a chunk and continuously copy them together on other devices\"},\n",
        "    {\"instruction\": \"What is mixed precision training, and how does it enhance training efficiency?\", \"input\": \"\", \"output\": \"memory efficiency\"},\n",
        "    {\"instruction\": \"Why does DeepSeek-V3 reduce precision only in specific parts of the model?\", \"input\": \"\", \"output\": \"heavy computations are executed\"},\n",
        "    {\"instruction\": \"What is the issue of overflow and underflow in low-precision training?\", \"input\": \"\", \"output\": \"impediment\"},\n",
        "    {\"instruction\": \"How does Fine-Grained Quantization help mitigate overflow and underflow?\", \"input\": \"\", \"output\": \"allows the each group of values to have a more suitable scaling factor\"},\n",
        "    {\"instruction\": \"Why is it important to store intermediate values in high precision?\", \"input\": \"\", \"output\": \"avoid that a lot of values with error are summed and their errors are accumulated\"},\n",
        "    {\"instruction\": \"What role does reinforcement learning play in DeepSeek-V3’s training process?\", \"input\": \"\", \"output\": \"gives feedback to the model and determine the direction of learning\"},\n",
        "    {\"instruction\": \"How does the reward model influence the learning direction of DeepSeek-V3?\", \"input\": \"\", \"output\": \"enhances model performance and reasoning capability\"},\n",
        "    {\"instruction\": \"What are the differences between rule-based and model-based reward models?\", \"input\": \"\", \"output\": \"enhances model performance and reasoning capability\"},\n",
        "    {\"instruction\": \"Why does DeepSeek-V3 include chain-of-thought reasoning in its reward model?\", \"input\": \"\", \"output\": \"Multi-Head Latent Attention\"},\n",
        "    {\"instruction\": \"What is Group Relative Policy Optimization (GRPO), and how does it improve reinforcement learning?\", \"input\": \"\", \"output\": \"updating the policy model π\"},\n",
        "    {\"instruction\": \"How does GRPO ensure that reinforcement learning does not cause the model to deviate too much from its base model?\", \"input\": \"\", \"output\": \"we pick minimum between the original policy and the clipped policy\"},\n",
        "    {\"instruction\": \"What is the purpose of KL divergence in GRPO?\", \"input\": \"\", \"output\": \"measures the difference between current policy model and reference policy model\"},\n",
        "    {\"instruction\": \"How does GRPO balance performance enhancement and stability?\", \"input\": \"\", \"output\": \"updating the policy model π\"},\n",
        "    {\"instruction\": \"What key advancements does DeepSeek-V3 bring to efficient training?\", \"input\": \"\", \"output\": \"cheaper GPUs\"},\n",
        "    {\"instruction\": \"How does DeepSeek-V3’s performance compare to models from OpenAI?\", \"input\": \"\", \"output\": \"It is unclear that its performance exceeds the OpenAI model\"},\n",
        "    {\"instruction\": \"What are the potential limitations of DeepSeek-V3 despite its innovations?\", \"input\": \"\", \"output\": \"efficiency and the speed of convergence during training\"},\n",
        "    {\"instruction\": \"How does DualPipe improve computation in machine learning?\", \"input\": \"\", \"output\": \"Reducing pipeline bubbles\"},\n",
        "    {\"instruction\": \"What is DualPipe?\", \"input\": \"\", \"output\": \"An innovative bidirectional pipeline parallelism algorithm\"},\n",
        "    {\"instruction\": \"What is the purpose of the table 'Pipeline Bubbles and Memory Usage Comparison'?\", \"input\": \"\", \"output\": \"Reducing pipeline bubbles\"},\n",
        "    {\"instruction\": \"Explain the terms '𝐹', '𝐵', and '𝑊' used in the DualPipe algorithm.\", \"input\": \"\", \"output\": \"PP, ZB1P\"},\n",
        "    {\"instruction\": \"Who developed DualPipe?\", \"input\": \"\", \"output\": \"Jiashi Li\"},\n",
        "    {\"instruction\": \"What is the role of the PyTorch Profiler in DeepSeek-V3?\", \"input\": \"\", \"output\": \"Capture training and inference profiling data\"},\n",
        "    {\"instruction\": \"How does the training profile data in DeepSeek-V3 optimize performance?\", \"input\": \"\", \"output\": \"Background training load\"},\n",
        "    {\"instruction\": \"What is the purpose of expert parallelism (EP) in DeepSeek-V3?\", \"input\": \"\", \"output\": \"Different experts are assigned to different GPUs\"},\n",
        "    {\"instruction\": \"What is the difference between Hierarchical and Global Load Balancing in the load balancing algorithm?\", \"input\": \"\", \"output\": \"Used when expert groups can be evenly packed into nodes\"},\n",
        "    {\"instruction\": \"How does the Fire-Flyer File System (3FS) support large-scale AI workloads?\", \"input\": \"\", \"output\": \"Leverages SSDs + RDMA networks\"},\n",
        "    {\"instruction\": \"What is KVCache and how does it optimize the LLM inference process?\", \"input\": \"\", \"output\": \"Key-value vectors from previous tokens to reduce redundant computations\"},\n",
        "    {\"instruction\": \"What performance results were achieved with the Fire-Flyer File System (3FS)?\", \"input\": \"\", \"output\": \"6.6 TiB/s read throughput\"},\n",
        "    {\"instruction\": \"What was the setup for the GraySort benchmark test using the Fire-Flyer File System?\", \"input\": \"\", \"output\": \"25 storage nodes + 50 compute nodes\"},\n",
        "    {\"instruction\": \"What makes Deepseek V3 a significant breakthrough in AI?\", \"input\": \"\", \"output\": \"Deepseek V3 introduces advanced efficiency optimizations, significantly reducing training costs while maintaining high performance.\"},\n",
        "    {\"instruction\": \"How does Deepseek V3 compare to other AI models in terms of cost efficiency?\", \"input\": \"\", \"output\": \"Deepseek V3 achieves a fraction of the training and inference costs compared to traditional dense models like GPT-4.\"},\n",
        "    {\"instruction\": \"What are the key architectural improvements in Deepseek V3?\", \"input\": \"\", \"output\": \"Deepseek V3 employs Mixture-of-Experts (MoE), FP8 mixed precision training, and Multi-head Latent Attention (MLA) for optimized performance.\"},\n",
        "    {\"instruction\": \"How did Deepseek V3 manage to make training 45 times more efficient?\", \"input\": \"\", \"output\": \"By using Mixture-of-Experts (MoE) and 8-bit precision instead of 32-bit, reducing computational and memory overhead.\"},\n",
        "    {\"instruction\": \"What is the Mixture-of-Experts (MoE) architecture used in Deepseek V3?\", \"input\": \"\", \"output\": \"MoE enables only 37 billion out of 236 billion total parameters to be active per token, improving efficiency.\"},\n",
        "    {\"instruction\": \"How does the FP8 mixed precision training improve memory efficiency?\", \"input\": \"\", \"output\": \"FP8 reduces memory usage by using lower precision representation while maintaining computational accuracy.\"},\n",
        "    {\"instruction\": \"What is the role of the Multi-head Latent Attention (MLA) in Deepseek V3?\", \"input\": \"\", \"output\": \"MLA helps compress the Key-Value cache, reducing memory overhead and improving inference efficiency.\"},\n",
        "    {\"instruction\": \"How does Deepseek V3’s training framework, HAI-LLM, enhance performance?\", \"input\": \"\", \"output\": \"HAI-LLM uses the DualPipe algorithm to reduce pipeline bubbles and optimize model parallelism.\"},\n",
        "    {\"instruction\": \"What strategies did Deepseek use to balance load in MoE models?\", \"input\": \"\", \"output\": \"Deepseek V3 uses hierarchical and global load balancing techniques to evenly distribute computation across GPUs.\"},\n",
        "    {\"instruction\": \"How does Deepseek V3's performance compare to GPT-4 and Claude 3.5 Sonnet?\", \"input\": \"\", \"output\": \"Deepseek V3 excels in reasoning and mathematical tasks, showing competitive results against GPT-4 and Claude 3.5 Sonnet.\"},\n",
        "    {\"instruction\": \"How much data was used to pre-train Deepseek V3?\", \"input\": \"\", \"output\": \"Deepseek V3 was trained on 14.8 trillion tokens.\"},\n",
        "    {\"instruction\": \"What are the estimated training costs for Deepseek V3?\", \"input\": \"\", \"output\": \"The estimated training cost for Deepseek V3 is approximately $5.5 million.\"},\n",
        "    {\"instruction\": \"How does Deepseek V3's training cost compare to Llama 3.1?\", \"input\": \"\", \"output\": \"Deepseek V3's training cost is significantly lower, requiring approximately 2.79 million GPU hours compared to Llama 3.1.\"},\n",
        "    {\"instruction\": \"What role did Nvidia H800 GPUs play in Deepseek V3’s training process?\", \"input\": \"\", \"output\": \"Deepseek V3 utilized Nvidia H800 GPUs to accelerate training with optimized compute efficiency for MoE models.\"},\n",
        "    {\"instruction\": \"How many GPU hours were required to train Deepseek V3?\", \"input\": \"\", \"output\": \"Deepseek V3 required approximately 2.79 million GPU hours for training.\"},\n",
        "    {\"instruction\": \"What optimizations did Deepseek use to reduce GPU memory usage?\", \"input\": \"\", \"output\": \"Deepseek V3 used 8-bit precision, mixed precision training, and efficient KV cache compression.\"},\n",
        "    {\"instruction\": \"How did Deepseek V3 achieve a 93% compression ratio for key-value indices?\", \"input\": \"\", \"output\": \"Through Multi-head Latent Attention (MLA), which optimizes KV cache storage and reduces redundancy.\"},\n",
        "    {\"instruction\": \"What are the advantages of multi-token prediction in Deepseek V3?\", \"input\": \"\", \"output\": \"Multi-token prediction improves inference efficiency by generating multiple tokens in a single step, reducing latency.\"},\n",
        "    {\"instruction\": \"Why is Deepseek V3 considered more efficient than traditional dense models?\", \"input\": \"\", \"output\": \"Its sparse activation, MoE architecture, and optimized training framework significantly reduce compute and memory requirements.\"},\n",
        "    {\"instruction\": \"How did Deepseek optimize cross-node communication in its training framework?\", \"input\": \"\", \"output\": \"Deepseek V3 employed high-speed interconnects and optimized tensor parallelism to minimize communication overhead.\"},\n",
        "    {\"instruction\": \"What is the significance of the $5.5 million cost estimate for Deepseek V3?\", \"input\": \"\", \"output\": \"It represents the estimated rental cost of GPUs and infrastructure required for training the model.\"},\n",
        "    {\"instruction\": \"How does Deepseek V3’s efficiency compare to Llama 3.1 in terms of FLOPs?\", \"input\": \"\", \"output\": \"Deepseek V3 achieves better real-world efficiency with fewer floating-point operations per second (FLOPs) per generated token.\"},\n",
        "    {\"instruction\": \"What assumptions were made in estimating the training cost of Deepseek V3?\", \"input\": \"\", \"output\": \"The estimate assumes an H800 GPU rental price of $2 per GPU hour and 2.79 million GPU hours.\"},\n",
        "    {\"instruction\": \"How did Deepseek reduce pipeline bubbles using the DualPipe algorithm?\", \"input\": \"\", \"output\": \"DualPipe optimizes bidirectional pipeline parallelism, reducing idle GPU time and improving efficiency.\"},\n",
        "    {\"instruction\": \"What was the estimated total number of FLOPs required for Deepseek V3’s training?\", \"input\": \"\", \"output\": \"The total estimated FLOPs required for Deepseek V3's training is approximately 2.79 million GPU hours.\"}\n",
        "\n",
        "]\n",
        "\n",
        "# Save as JSONL file\n",
        "file_path = \"qa_data.jsonl\"\n",
        "with open(file_path, \"w\") as f:\n",
        "    for entry in qa_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"JSONL file saved as {file_path}\")\n",
        "\n",
        "# Download the JSONL file\n",
        "from google.colab import files\n",
        "files.download(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QRMdme3K1f5P",
        "outputId": "116ee494-2943-48cc-d84a-1edf7253aaea"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSONL file saved as qa_data.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16b766f2-6f3a-4507-932f-2a8567f59f80\", \"qa_data.jsonl\", 14129)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eiv_7G_B2GVT",
        "outputId": "d941529c-481d-44a2-d74f-9d180a3ca435"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"qa_data.jsonl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5ec42c1e7eaf4fb0adf9ff3ddb89bd12",
            "502fcdfaa53d499babf60bcab31f98e6",
            "796ce2a9ca9f4e71856c29871f4c2345",
            "1c7848216a5f496aabfeb58d7f8b17af",
            "e655754ca7ec4197862377e379e0b984",
            "dd194b867a954c0abb9162831f69d148",
            "d12d3b2b69a44322b4152cb5ab819c99",
            "08ef96f9a30c461289c8805da775c27f",
            "b13d1fb3eac34d6f84a9d70fd66b714f",
            "3f59d0d91c07478fa0eb88306471d59d",
            "ec174e0791264fbe9433f0ca518ea0e4"
          ]
        },
        "id": "AZLxkRe-2KWT",
        "outputId": "6776dd64-c8fe-4d40-d69b-3388dbf53341"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec42c1e7eaf4fb0adf9ff3ddb89bd12"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9qVPct92VVZ",
        "outputId": "ef1dcea6-bef6-42eb-8763-08a7e85d0bc2"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  9 15:13:08 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0             27W /   70W |    9006MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate bitsandbytes peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehnsc0E82ifq",
        "outputId": "6b2602aa-2466-41a1-b986-2a12b9a7afb3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0a0f3a1dff1f4868927fa29c3c2dd8ac",
            "913acec0d02d4e5c8cea25b1e118495b",
            "3e92448a09dd49c5bfb1076daa17cb99",
            "647869f286fe4508a19eda2e6a7baebe",
            "0f0095ccd5f3447aa0254c4220743481",
            "8bd43f5607fb48b7b1fac3ef833e435b",
            "7b4c2160756b45dc9589bceb97b009db",
            "35c67109afcf4eb1a3e4563d0d01479c",
            "d2b3f505defc491e8abb1502b3da4243",
            "50ed75d870194237b32f31c8651ba96c",
            "b1aa5229dcc146ef95d91a96af1a610c"
          ]
        },
        "id": "pB7hCRFZ29wB",
        "outputId": "21c3362b-ab1d-45cb-988e-3ebccbded1d7"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a0f3a1dff1f4868927fa29c3c2dd8ac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "zPz9xktp38kV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load your dataset (Make sure the file exists in Colab's working directory)\n",
        "dataset = load_dataset(\"json\", data_files=\"qa_data.jsonl\")[\"train\"]\n",
        "\n",
        "# Manually split into train and eval (90% train, 10% eval)\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]  # This will be used for evaluation\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"instruction\"], examples[\"output\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_datasets = split_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Assign the tokenized datasets\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"test\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9cafebbc0afd4423a2b7128804ed624c",
            "4abd9caf7c994b1bb87ad68bdf09f4c2",
            "4b84c97ad8f841a5be5dddfa2acd92c3",
            "3af39c0edd834eed90b2cf0138ce3894",
            "d3576b44d89a449b9ed8efb1cd94699e",
            "6ffce64625974681b642def452b491f9",
            "ac5ca25f818547a2a056c9515983364e",
            "c7d5fa26830b469d8d736131a464d73c",
            "c5048cdc1dca4215a4d93de6504289f0",
            "432a69b37fa6453099a0f31500fcb3df",
            "0b41727b85b2459b97f88ce1114c3c47",
            "d8c8f341c4f94d888ca75907d48332ef",
            "0af35ce5c86e4b6fbe0460ca86145045",
            "a0763a02632e4fb4b159dc65a3b0c8e1",
            "788aac0174184dc780af9806c06dcc4a",
            "67ba0ac430a94a7aab502ef88e121733",
            "d095bc1c758d48d1b8805f774950c8ad",
            "c2c4f5f7210b4c8ebcdeed955ea77b33",
            "e4caba4df19c4ef7986ae57edd7d1da1",
            "de880ef7875a4e1cb8b0c30be5b10dba",
            "bd2d2dc3fe2c4219902942e44c63a47f",
            "2263a363709940798501c7ea7ff829f7"
          ]
        },
        "id": "ZTMPVx9i7qUb",
        "outputId": "3cf60245-6595-4f8f-8eba-0144706883a7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cafebbc0afd4423a2b7128804ed624c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8c8f341c4f94d888ca75907d48332ef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Load a non-quantized version of the model (IMPORTANT)\n",
        "model_name = \"Qwen/Qwen2.5-3B\"  # Update if using a different version\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,  # Use bf16 instead of fp16\n",
        "    device_map=\"auto\"  # Let Hugging Face auto-allocate the model\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Apply LoRA (Parameter-Efficient Fine-Tuning)\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=8,  # Rank\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # Update this based on your model\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ✅ Make sure LoRA layers are trainable\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "print(\"✅ LoRA fine-tuning setup completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "fe032768b380479f94bfb1c3dcf93daf",
            "1ae6ee35ddda45568dca77dec0d652e9",
            "c2e36f4fb0e1433da49367ea89a8e962",
            "5890d46a58d3432299958e139c8001eb",
            "cb32363bfc5f4e5eaf95ffa0003eab99",
            "91e3b3fbe12f43789d1beb1addab5276",
            "854fd491d1f447e2b4411986045a1afa",
            "ab88575135084b15a71403ed9b78d7f9",
            "ac50bd830c594a27af8539a6a78168b0",
            "4ab06350a1744a3c9314d659a9753ca5",
            "afa1d19d5f9e48d8874d9508793f369a"
          ]
        },
        "id": "7-1mteAm4JqX",
        "outputId": "9dd322b7-051c-473b-e8f4-f2ad2285b74b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe032768b380479f94bfb1c3dcf93daf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LoRA fine-tuning setup completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./qwen-lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"./qwen-lora-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfMppgeHBUlN",
        "outputId": "2f1303ee-a3d9-49b9-fc22-1d0c574a8c9b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./qwen-lora-finetuned/tokenizer_config.json',\n",
              " './qwen-lora-finetuned/special_tokens_map.json',\n",
              " './qwen-lora-finetuned/vocab.json',\n",
              " './qwen-lora-finetuned/merges.txt',\n",
              " './qwen-lora-finetuned/added_tokens.json',\n",
              " './qwen-lora-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r qwen-lora-finetuned.zip ./qwen-lora-finetuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUUk6Hpmdcv9",
        "outputId": "02ad167f-cb82-422f-c80b-f6cd7052d3ef"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: qwen-lora-finetuned/ (stored 0%)\n",
            "updating: qwen-lora-finetuned/added_tokens.json (deflated 67%)\n",
            "updating: qwen-lora-finetuned/README.md (deflated 66%)\n",
            "updating: qwen-lora-finetuned/vocab.json (deflated 61%)\n",
            "updating: qwen-lora-finetuned/tokenizer.json (deflated 81%)\n",
            "updating: qwen-lora-finetuned/tokenizer_config.json (deflated 83%)\n",
            "updating: qwen-lora-finetuned/adapter_model.safetensors (deflated 70%)\n",
            "updating: qwen-lora-finetuned/special_tokens_map.json (deflated 69%)\n",
            "updating: qwen-lora-finetuned/adapter_config.json (deflated 53%)\n",
            "updating: qwen-lora-finetuned/merges.txt (deflated 57%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yyDCTB0-exhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"qwen-lora-finetuned.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WyP2PbNffOy9",
        "outputId": "6022bfac-c85f-49f3-dbb3-eefd12886ce5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1fec12ee-9639-4c24-af41-26b3bcc4e8fb\", \"qwen-lora-finetuned.zip\", 6142493)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "nhqKSt6VBYvg",
        "outputId": "5a4cb93a-d219-407c-e289-83e5ac9e7f27"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-f245b31d31e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who developed DualPipe?\"\n",
        "input_ids = tokenizer(question, return_tensors=\"pt\").input_ids.cuda()\n",
        "output = model.generate(input_ids, max_length=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "r0o5XwaDBbXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}